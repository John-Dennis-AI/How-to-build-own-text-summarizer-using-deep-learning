{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abstractive Text Summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/John-Dennis-AI/How-to-build-own-text-summarizer-using-deep-learning/blob/master/Abstractive_Text_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFuL-RBgXqgU",
        "colab_type": "text"
      },
      "source": [
        "# Abstractive Text Summarizer\n",
        "\n",
        "This notebook demonstrates the concept of text abstraction for summarizing an article. ***Abstraction*** differs from ***extraction*** in that the former method uses context to produce a new summary as opposed to the latter method that uses exact words and phrases from the original article. Abstraction is not necessarily a subset of the original text, but extraction is a subset of the original text.\n",
        "\n",
        "In this notebook, we will build a deep learning text abstraction model using Python and Keras to generate a summary from the Amazon Kindle Store book reviews. If you are interested, you can download the dataset from [here](https://www.kaggle.com/bharadwaj6/kindle-reviews).\n",
        "\n",
        "The original text reviews are the source and the original summaries are the target. The model builds \"vocabulary dictionaries\" out of both the text reviews and the summaries, and then learns from observation how to construct a summary given a text review so that it can operate on unseen text reviews by tapping into the original summary vocabulary dictionary. The model looks for certain key words in the text reviews, then it goes to the summary vocabulary dictionary and finds the word with the highest probability of being the next word and produces that word to build a predicted summary.\n",
        "\n",
        "**Significant credit for this notebook goes to the original author for his wonderful work**, located [here](https://colab.research.google.com/github/aravindpai/How-to-build-own-text-summarizer-using-deep-learning/blob/master/How_to_build_own_text_summarizer_using_deep_learning.ipynb).\n",
        "\n",
        "I made my own edits and modifications as part of my ongoing learning process, which includes the following items:\n",
        "\n",
        "*   I used a completely different dataset than used in the original work\n",
        "*   I made alterations to the embedding layers of the Encoder\n",
        "*   I made code updates to satisfy Python requirements\n",
        "*   I added several updates to notes and comments throughout for added clarity\n",
        "*   I included the addition of a BLEU score to measure model performance\n",
        "\n",
        "I used a GPU for model development, but you can experiment with a CPU, GPU, or TPU to see what works best for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5dSoP8lGMZi",
        "colab_type": "text"
      },
      "source": [
        "#Using a Custom Attention Layer\n",
        "\n",
        "Attention layers are helpful for text summarization. They take a small sample of words from a long sentence and summarize it, then they take another sample, and so on. They use vectors to find relationships between words, assign a relevancy, and ignore the less relevant words. Keras does not officially support attention layers, so we are borrowing some code to implement it. Credit for the original work and code is located [here](https://github.com/thushv89/attention_keras/blob/master/layers/attention.py).\n",
        "\n",
        "Here's where we import the attention layer into our environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikKHccKQI49o",
        "colab_type": "code",
        "outputId": "1446b7e0-1b3d-4c87-e6d1-c39f58d0966e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUValOzcHtEK",
        "colab_type": "text"
      },
      "source": [
        "#Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "_Jpu8qLEFxcY",
        "colab_type": "code",
        "outputId": "b686d09a-ddd9-4672-ec7e-32a3a29a0676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np              #performs fast matrix math\n",
        "import pandas as pd             #data analysis library that uses data frames (similar to spreadsheets)\n",
        "import re                       #library for regular expressions\n",
        "from bs4 import BeautifulSoup   #BeautifulSoup is a parsing package\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "from nltk.translate.bleu_score import sentence_bleu  #used for calculating BLEU score\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVakjZ3oICgx",
        "colab_type": "text"
      },
      "source": [
        "#Read the Dataset\n",
        "\n",
        "This dataset consists of book reviews from Amazon's Kindle Store. The data spans from 1996 to 2014 and has over 900,000 reviews. These reviews include product and user information, ratings, plain text reviews, and summaries. We'll be focusing on the text reviews and summaries.\n",
        "\n",
        "We’ll take a sample of 25,000 reviews to reduce the training time of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wnK5o4Z1Fxcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#if using Google Colab, files are stored in the default files folder (file icon in left pane)\n",
        "#files must be uploaded every time there is a new run\n",
        "\n",
        "data=pd.read_csv(\"kindle_reviews.csv\",nrows=25000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGNQKvCaISIn",
        "colab_type": "text"
      },
      "source": [
        "# Remove Duplicates and NA Values\n",
        "Here is where we do some initial basic data cleaning. We remove duplicates and invalid values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Cjul88oOFxcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop_duplicates(subset=['reviewText'],inplace=True)  #remove duplicates\n",
        "data.dropna(axis=0,inplace=True)                    #remove na values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi0xD6BkIWAm",
        "colab_type": "text"
      },
      "source": [
        "# Information about the Dataset\n",
        "\n",
        "Now let's take a look at the datatypes and the shape of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "__fy-JxTFxc9",
        "colab_type": "code",
        "outputId": "d1c19dfe-6bb7-46cd-e2d1-4d102e2e4c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 24946 entries, 0 to 24999\n",
            "Data columns (total 10 columns):\n",
            "Unnamed: 0        24946 non-null int64\n",
            "asin              24946 non-null object\n",
            "helpful           24946 non-null object\n",
            "overall           24946 non-null int64\n",
            "reviewText        24946 non-null object\n",
            "reviewTime        24946 non-null object\n",
            "reviewerID        24946 non-null object\n",
            "reviewerName      24946 non-null object\n",
            "summary           24946 non-null object\n",
            "unixReviewTime    24946 non-null int64\n",
            "dtypes: int64(3), object(7)\n",
            "memory usage: 2.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0xLYACiFxdJ",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing the Data\n",
        "\n",
        "In this step, we continue to clean the data by removing all the unwanted symbols, characters, etc. from the text that will have no impact on our desired results. In particular, this is the dictionary that we will use for expanding contractions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0s6IY-x2FxdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JFRXFHmI7Mj",
        "colab_type": "text"
      },
      "source": [
        "#And More Preprocessing of the Data\n",
        "\n",
        "Here, we continue to clean up the data by doing the following:\n",
        "\n",
        "1.Convert everything to lowercase\n",
        "\n",
        "2.Remove HTML tags\n",
        "\n",
        "3.Contraction mapping\n",
        "\n",
        "4.Remove (‘s)\n",
        "\n",
        "5.Remove any text inside parenthesis ( )\n",
        "\n",
        "6.Eliminate punctuations and special characters\n",
        "\n",
        "7.Remove stopwords\n",
        "\n",
        "8.Remove short words\n",
        "\n",
        "And here's the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XZr-u3OEFxdT",
        "colab_type": "code",
        "outputId": "7a7c64a0-b67e-4906-bfcd-ed549195a6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def text_cleaner(text,num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
        "    if(num==0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens=newString.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>1:               #here we remove short words\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "A2QAeCHWFxdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now call the text_cleaner function on the Text field\n",
        "cleaned_text = []\n",
        "for t in data['reviewText']:\n",
        "    cleaned_text.append(text_cleaner(t,0)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snRZY8wjLao2",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at the first ten preprocessed reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NCAIkhWbFxdh",
        "colab_type": "code",
        "outputId": "77694b34-6844-41ce-ed80-d753a8da63dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "cleaned_text[:10]  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['enjoy vintage books movies enjoyed reading book plot unusual think killing someone self defense leaving scene body without notifying police hitting someone jaw knock would wash today still good read',\n",
              " 'book reissue old one author born era say nero wolfe introduction quite interesting explaining author forgotten would never heard language little dated times like calling gun heater also made good use fire dictionary look words like deshabille canarsie still well worth look see',\n",
              " 'fairly interesting read old style terminology glad get read story coarse crasslanguage read fun relaxation like free ebooksbecause check writer decide intriguing innovative enough command englishthat convey story without crude language',\n",
              " 'would never read amy brewster mysteries one really hooked',\n",
              " 'like period pieces clothing lingo enjoy mystery author guessing least way',\n",
              " 'beautiful depth character description makes like fast pacing movie pity mr merwin write instead amy brewster mysteries',\n",
              " 'enjoyed one tho sure called amy brewster mystery much clean well written characters well drawn',\n",
              " 'never heard amy brewster need like amy brewster like book actually amy brewster side kick story added mystery story one resolved story brings back old times simple life simple people straight relationships',\n",
              " 'darth maul working cloak darkness committing sabotage story worth reading many times great story',\n",
              " 'short story focused darth maul role helping trade federation gain mining colony bad also nothing exceptional fairly short really get see characters develop events happen seem go quickly including major battles story included novelshadow hunter worth reading bother buy one separately']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GsRXocxoFxd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#call the text_cleaner function on the Summary field\n",
        "cleaned_summary = []\n",
        "for t in data['summary']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZeD0gs6Lnb-",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at the first 10 preprocessed summaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jQJdZcAzFxee",
        "colab_type": "code",
        "outputId": "a3633e3d-8f4b-4f76-c8b2-25aef27b902a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "cleaned_summary[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nice vintage story',\n",
              " 'different',\n",
              " 'oldie',\n",
              " 'really liked it',\n",
              " 'period mystery',\n",
              " 'review',\n",
              " 'nice old fashioned story',\n",
              " 'enjoyable reading and reminding the old times',\n",
              " 'darth maul',\n",
              " 'not bad not exceptional']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "L1zLpnqsFxey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['cleaned_text']=cleaned_text\n",
        "data['cleaned_summary']=cleaned_summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT_D2cLiLy77",
        "colab_type": "text"
      },
      "source": [
        "#Remove the Empty Rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sYK390unFxfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm8Fk2TCL7Sp",
        "colab_type": "text"
      },
      "source": [
        "#Understanding the Distribution of the Sequences\n",
        "\n",
        "Here, we analyze the length of the reviews and the summary to get an idea of the distribution of the length of the text. This will help us determine the maximum length of the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MdF76AHHFxgw",
        "colab_type": "code",
        "outputId": "f53efaae-5710-43af-8414-c2f0162468ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in data['cleaned_text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in data['cleaned_summary']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "\n",
        "length_df.hist(bins = 50) #more bins per graph is more granular\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOOmbmRWIk76ZWYH8fzha0YnFD/HTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwdSGIhGMEbz",
        "colab_type": "text"
      },
      "source": [
        "It appears that the majority of summaries are less than a length of 8, so that would make a reasonable cutoff point.\n",
        "\n",
        "Just for fun, let's see the percentage of summaries with a length less than or equal to 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7JRjwdIOFxg3",
        "colab_type": "code",
        "outputId": "c5a8c538-11b0-4715-fcb7-d5b5e37e6c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cnt=0\n",
        "for i in data['cleaned_summary']:\n",
        "    if(len(i.split())<=8):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(data['cleaned_summary']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9478533778618276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYB4Ga9KMjEu",
        "colab_type": "text"
      },
      "source": [
        "We've confirmed that nearly 95% of the summaries have a length below 8.\n",
        "\n",
        "Looking at the graph, it seems that 70 is a reasonable cutoff for the text reviews. Let's test it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gaZc6gMPPGB",
        "colab_type": "code",
        "outputId": "64855b1c-d118-449c-9560-117cd257d94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cnt=0\n",
        "for i in data['cleaned_text']:\n",
        "    if(len(i.split())<=70):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(data['cleaned_text']))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7681165251679878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ETx8dH0QjOb",
        "colab_type": "text"
      },
      "source": [
        "We see that a length of 70 gives us nearly 77% of the text reviews. That is more than we need, so we set the text=70 and the summary=8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZKD5VOWqFxhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_text_len=50\n",
        "max_summary_len=8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6d48E-8M4VO",
        "colab_type": "text"
      },
      "source": [
        "Let's select the text reviews and summaries whose length falls below or equal to **max_text_len** and **max_summary_len**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yY0tEJP0FxhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_text =np.array(data['cleaned_text'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR1uh8xSNUma",
        "colab_type": "text"
      },
      "source": [
        "Here we add the **START** and **END** special tokens at the beginning and end of the summary. Here, I have chosen **sostok** and **eostok** as START and END tokens.\n",
        "\n",
        "**Note:** Be sure that the chosen special tokens never appear in the summary, so try to use something unique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EwLUH78CFxhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GlcX4RFOh13",
        "colab_type": "text"
      },
      "source": [
        "Now we need to split our dataset into a training and validation (testing) set. We’ll use 80% of the dataset as the training data and evaluate the performance on the remaining 20% (holdout set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RakakKHcFxhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.2,random_state=0,shuffle=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq1mqyOHOtIl",
        "colab_type": "text"
      },
      "source": [
        "#Preparing the Tokenizer\n",
        "\n",
        "A tokenizer builds the vocabulary and converts a word sequence to an integer sequence. Here's where we build tokenizers for the text reviews and summaries.\n",
        "\n",
        "#Text Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oRHTgX6hFxhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences  #pads sequences so they are the same length\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzvLwYL_PDcx",
        "colab_type": "text"
      },
      "source": [
        "#Calculating Rare Words in Text Reviews\n",
        "\n",
        "Let's look at the number of rare words in the text reviews.\n",
        "\n",
        "Here, I am defining the threshold to be 4 which means a word with a frequency count below 4 is considered to be a rare word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "y8KronV2Fxhx",
        "colab_type": "code",
        "outputId": "19c10612-8539-4ced-b814-ed3690284c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "#freq=0\n",
        "#tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    #tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        #freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "#print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 67.32584269662921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So-J-5kzQIeO",
        "colab_type": "text"
      },
      "source": [
        "**For Reference**:\n",
        "\n",
        "\n",
        "* **tot_cnt** gives the size of vocabulary (which means every unique word in the text)\n",
        " \n",
        "*   **cnt** gives the number of rare words whose count falls below the threshold\n",
        "\n",
        "*  **tot_cnt - cnt** gives the top most common words (with rare words removed)\n",
        "\n",
        "Let's define the tokenizer with the top most common words for text reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "J2giEsF3Fxh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero up to maximum length\n",
        "x_tr = pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc = x_tokenizer.num_words + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DCbGMsm4FxiA",
        "colab_type": "code",
        "outputId": "f85c2c4e-6b94-4f2c-82e5-2c5ff3eddcbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_voc"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQfKP3sqRxi9",
        "colab_type": "text"
      },
      "source": [
        "#Summary Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eRHqyBkBFxiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KInA6O6ZSkJz",
        "colab_type": "text"
      },
      "source": [
        "#Calculating Rare Words in Summaries\n",
        "\n",
        "Let's look at the number of rare words in the summaries.\n",
        "\n",
        "Here, I am defining the threshold to be 6 which means a word with a frequency count below 6 is considered to be a rare word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yzE5OiRLFxiM",
        "colab_type": "code",
        "outputId": "85a7c48d-842f-4f42-a22e-a04ebca96ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "thresh=6\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "#freq=0\n",
        "#tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    #tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        #freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "#print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 83.38150289017341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PBhzKuRSw_9",
        "colab_type": "text"
      },
      "source": [
        "Let's define the tokenizer with the top most common words for summaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-fswLvIgFxiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "#padding zero up to maximum length\n",
        "y_tr = pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc = y_tokenizer.num_words +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDLlCO56xVzc",
        "colab_type": "code",
        "outputId": "d56b1996-ad13-40dc-a826-00f156fd0047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_voc"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "806"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqwDUT5oTFmn",
        "colab_type": "text"
      },
      "source": [
        "Let's check whether the word count of the start token is equal to length of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pR8IX9FRFxiY",
        "colab_type": "code",
        "outputId": "a2f26fa0-e4b3-4f37-e5d3-3de2f266a620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_tokenizer.word_counts['sostok'],len(y_tr)   "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12906, 12906)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVFhFVguTTtw",
        "colab_type": "text"
      },
      "source": [
        "Here, we delete the rows that contain only **START** and **END** tokens for both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kZ-vW82sFxih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cx5NISuMFxik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOtlDcthFxip",
        "colab_type": "text"
      },
      "source": [
        "#Building the Model\n",
        "\n",
        "We are finally at the model building part. But before we do that, we need to familiarize ourselves with a few important terms.\n",
        "\n",
        "**Return_Sequences = True**: When the return_sequences parameter is set to True, LSTM produces the hidden state and cell state for every timestep\n",
        "\n",
        "**Return_State = True**: When return_state = True, LSTM produces the hidden state and cell state of the last timestep only\n",
        "\n",
        "**Initial_State**: This is used to initialize the internal states of the LSTM for the first timestep\n",
        "\n",
        "**Stacked LSTM**: This is not a parameter, but a concept. A Stacked LSTM has multiple layers of LSTM stacked on top of each other. This leads to a better representation of the sequence. You may wish to experiment with different multiple layers of the LSTM stacked on top of each other to see what you get.\n",
        "\n",
        "Here, we are building a 5 layer Stacked LSTM for the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zXef38nBFxir",
        "colab_type": "code",
        "outputId": "22101bd6-bab6-4bb7-8db0-d5fd114aaa65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "from keras import backend as K \n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "# Start of the Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "# Embedding layer for the Encoder\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "# Encoder lstm 1\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# Encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# Encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output3, state_h3, state_c3= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Encoder lstm 4\n",
        "encoder_lstm4=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output4, state_h4, state_c4= encoder_lstm4(encoder_output3)\n",
        "\n",
        "# Encoder lstm 5\n",
        "encoder_lstm5=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm5(encoder_output4)\n",
        "\n",
        "# Start of the Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# Embedding layer for the Decoder\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Set up the Decoder, using Encoder lstm 5 output as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention layer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention input and Decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary() "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 50, 100)      581700      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 50, 300), (N 481200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 50, 300), (N 721200      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 50, 300), (N 721200      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 50, 300), (N 721200      lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    80600       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 50, 300), (N 721200      lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
            "                                                                 lstm_4[0][1]                     \n",
            "                                                                 lstm_4[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_4[0][0]                     \n",
            "                                                                 lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 600)    0           lstm_5[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 806)    484406      concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 5,174,206\n",
            "Trainable params: 5,174,206\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZVlfRuMUcoP",
        "colab_type": "text"
      },
      "source": [
        "The rmsprop optimizer is a type of gradient descent algorithm. Also, we are using sparse_categorical_crossentropy as the loss function since it converts the integer sequence to a one-hot vector on the fly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Lwfi1Fm8Fxiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0ykDbxfUhyw",
        "colab_type": "text"
      },
      "source": [
        "Here we use early stopping as a regulariziation technique. We will monitor the validation loss (val_loss), and the model will stop training once the validation loss increases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-A3J92MUljB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw6CVECaUq5b",
        "colab_type": "text"
      },
      "source": [
        "We’ll train the model on a batch size of 128 and validate it on the holdout set (which is 20% of our dataset):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ETnPzA4OFxi3",
        "colab_type": "code",
        "outputId": "320a8447-084a-45b6-868a-6272f0f96f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 12051 samples, validate on 3013 samples\n",
            "Epoch 1/50\n",
            "12051/12051 [==============================] - 66s 5ms/sample - loss: 2.6121 - val_loss: 2.2459\n",
            "Epoch 2/50\n",
            "12051/12051 [==============================] - 58s 5ms/sample - loss: 2.2001 - val_loss: 2.1323\n",
            "Epoch 3/50\n",
            "12051/12051 [==============================] - 59s 5ms/sample - loss: 2.1074 - val_loss: 2.0858\n",
            "Epoch 4/50\n",
            "12051/12051 [==============================] - 59s 5ms/sample - loss: 2.0490 - val_loss: 2.0709\n",
            "Epoch 5/50\n",
            "12051/12051 [==============================] - 59s 5ms/sample - loss: 2.0032 - val_loss: 2.0276\n",
            "Epoch 6/50\n",
            "12051/12051 [==============================] - 58s 5ms/sample - loss: 1.9631 - val_loss: 1.9982\n",
            "Epoch 7/50\n",
            "12051/12051 [==============================] - 58s 5ms/sample - loss: 1.9323 - val_loss: 1.9704\n",
            "Epoch 8/50\n",
            "12051/12051 [==============================] - 57s 5ms/sample - loss: 1.8983 - val_loss: 1.9559\n",
            "Epoch 9/50\n",
            "12051/12051 [==============================] - 56s 5ms/sample - loss: 1.8699 - val_loss: 1.9346\n",
            "Epoch 10/50\n",
            "12051/12051 [==============================] - 56s 5ms/sample - loss: 1.8414 - val_loss: 1.9263\n",
            "Epoch 11/50\n",
            "12051/12051 [==============================] - 56s 5ms/sample - loss: 1.8160 - val_loss: 1.9365\n",
            "Epoch 12/50\n",
            "12051/12051 [==============================] - 57s 5ms/sample - loss: 1.7926 - val_loss: 1.9126\n",
            "Epoch 13/50\n",
            "12051/12051 [==============================] - 56s 5ms/sample - loss: 1.7657 - val_loss: 1.9126\n",
            "Epoch 14/50\n",
            "12051/12051 [==============================] - 56s 5ms/sample - loss: 1.7423 - val_loss: 1.8986\n",
            "Epoch 15/50\n",
            "12051/12051 [==============================] - 55s 5ms/sample - loss: 1.7190 - val_loss: 1.8879\n",
            "Epoch 16/50\n",
            "12051/12051 [==============================] - 55s 5ms/sample - loss: 1.6958 - val_loss: 1.9045\n",
            "Epoch 17/50\n",
            "12051/12051 [==============================] - 55s 5ms/sample - loss: 1.6744 - val_loss: 1.8890\n",
            "Epoch 00017: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ezKYOp2UxG5",
        "colab_type": "text"
      },
      "source": [
        "#Understanding the Loss Plot\n",
        "\n",
        "Now, we will produce a diagnostic plot to understand the behavior of the model over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tDTNLAURFxjE",
        "colab_type": "code",
        "outputId": "714af98d-ca51-40ff-d12f-a1644a132816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIY/WVN2qvBgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSyx-HvpUz2o",
        "colab_type": "text"
      },
      "source": [
        "From the plot, we can infer that validation (test) loss has increased after epoch 15 for 2 successive epochs. Hence, training is stopped at epoch 17.\n",
        "\n",
        "Next, let’s build the dictionary to convert the index to word for the target and source vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sBX0zZnOFxjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word    #integers with their words for original summary\n",
        "reverse_source_word_index=x_tokenizer.index_word    #integers with their words for original text reviews\n",
        "target_word_index=y_tokenizer.word_index            #a dictionary of words and their uniquely assigned integers for original summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM_nU_VvFxjq",
        "colab_type": "text"
      },
      "source": [
        "# Inference\n",
        "\n",
        "Inference means that we have settled on a model with the best parameters, and now we are ready to run it on new, unseen data. Here, we set up the inference for the encoder and decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9QkrNV-4Fxjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# Attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRWXa17uIshl",
        "colab_type": "text"
      },
      "source": [
        "The function below is the implementation of the inference process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6f6TTFnBFxj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input (text reviews) as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence (summaries) with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        # Here is where the model looks for candidate words\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        # Build the new summary based on the original summary vocabulary\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GuDf4TPWt6_",
        "colab_type": "text"
      },
      "source": [
        "Let's define the functions to convert an integer sequence to a word sequence for the summaries as well as the text reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aAUntznIFxj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gM4ALyfWwA9",
        "colab_type": "text"
      },
      "source": [
        "Following are a few summaries generated by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BUtQmQTmFxkI",
        "colab_type": "code",
        "outputId": "799f70e7-45de-47c3-bd65-1aacc58edc1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bscore=0\n",
        "tot_bscore=0\n",
        "\n",
        "for i in range(0,25):\n",
        "    print(\"Original Text Review:\",seq2text(x_tr[i]))\n",
        "    print(\"Original Summary:\",seq2summary(y_tr[i]))\n",
        "    print(\"Predicted Summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
        "    references = seq2summary(y_tr[i])\n",
        "    candidates = decode_sequence(x_tr[i].reshape(1,max_text_len))\n",
        "    bscore = sentence_bleu(references, candidates)\n",
        "    tot_bscore += bscore\n",
        "    print(\"BLEU score:\", bscore)\n",
        "    print(\"\\n\")\n",
        "  \n",
        "print(\"Average of All BLEU Scores:\", tot_bscore/(i+1))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Text Review: favorite worse either books short story realize quick read easy follow kept interest \n",
            "Original Summary: perfect crime \n",
            "Predicted Summary:  not bad\n",
            "BLEU score: 0.7071067811865476\n",
            "\n",
            "\n",
            "Original Text Review: great middle day read stumbled across actually liked interesting held interest makes great book someone likes read cannot book right \n",
            "Original Summary: simple \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.668740304976422\n",
            "\n",
            "\n",
            "Original Text Review: hoping next set anthologies keep one actually intrigued stories definitely go ahead get graphic enough plots keep going \n",
            "Original Summary: nice \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.668740304976422\n",
            "\n",
            "\n",
            "Original Text Review: sizzling cover last page minx malone makes read well worth effort co workers best friends drawn makes laugh sigh like really great movie sit watch hope might sequel hope becca \n",
            "Original Summary: well worth the read \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.8801117367933934\n",
            "\n",
            "\n",
            "Original Text Review: easy read love pride prejudice stories touch night must read jane austen fan \n",
            "Original Summary: fun \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.5623413251903491\n",
            "\n",
            "\n",
            "Original Text Review: loved author excellent job giving us characters could people know sophie tucker comes family believes share talents becomes reader instead early hours morning hooked know found smiling sophie makes life change promises bring true happiness \n",
            "Original Summary: you go girl \n",
            "Predicted Summary:  great read\n",
            "BLEU score: 0.7226568811456053\n",
            "\n",
            "\n",
            "Original Text Review: really feeling story trained never get hopes nn books feel connection feel romance bed play lukewarm story time killer easily forgettable go ahead see read description trip memory literally thinking read yet found story asked actually read feeling need review read description would forgotten story together \n",
            "Original Summary: meh it is okay \n",
            "Predicted Summary:  not bad\n",
            "BLEU score: 0.8408964152537145\n",
            "\n",
            "\n",
            "Original Text Review: course try story since love fairy tales synopsis promised pleasant escapist fantasy story deliver thumbs beauty beast angle good start promising new fantasy series \n",
            "Original Summary: cute \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.668740304976422\n",
            "\n",
            "\n",
            "Original Text Review: definitely hot read heroine likeable nice overbearing jerk really close line tension really well crafted though \n",
            "Original Summary: bit \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.5623413251903491\n",
            "\n",
            "\n",
            "Original Text Review: want say nice short little story short story nice little plot enough romance \n",
            "Original Summary: nice read \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.8408964152537145\n",
            "\n",
            "\n",
            "Original Text Review: love plenty light sure people saying give enough maybe easily please would recommend \n",
            "Original Summary: perfect \n",
            "Predicted Summary:  great read\n",
            "BLEU score: 0.7765453555044466\n",
            "\n",
            "\n",
            "Original Text Review: downloaded free one star might valuable information book difficult use read like pulling teeth collection old blog posts nothing \n",
            "Original Summary: horrible \n",
            "Predicted Summary:  not bad\n",
            "BLEU score: 0.7825422900366437\n",
            "\n",
            "\n",
            "Original Text Review: read book days good storyline good pace love fact book kindle definitely going back read sequels highly recommend anyone books like \n",
            "Original Summary: pleasant \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.7400828044922853\n",
            "\n",
            "\n",
            "Original Text Review: possible way give review worthy writing book could never justice compelling deep dark seriously twisted story descriptive characters truly come life feel every joy every gut wrenching act sadness every moment gifted writer aside lost never disappoints \n",
            "Original Summary: incredible \n",
            "Predicted Summary:  not my cup of tea\n",
            "BLEU score: 0.6865890479690392\n",
            "\n",
            "\n",
            "Original Text Review: read yet ones read kid held well better fond memories made mistake reading must fan howard characters \n",
            "Original Summary: still good \n",
            "Predicted Summary:  the\n",
            "BLEU score: 0.8408964152537145\n",
            "\n",
            "\n",
            "Original Text Review: bought series took forever read dont know started hooked cant wait book one bomb love series cant go wrong give try wont disappointed \n",
            "Original Summary: favorite author \n",
            "Predicted Summary:  not my cup of tea\n",
            "BLEU score: 0.7896895367562644\n",
            "\n",
            "\n",
            "Original Text Review: read many elizabeth lennox books cannot remember without reading write review right know great read also elizabeth books great \n",
            "Original Summary: the italian deal \n",
            "Predicted Summary:  the\n",
            "BLEU score: 1.0\n",
            "\n",
            "\n",
            "Original Text Review: given tastes reading habits change years could think would found humor mildly amusing back reminded old short stories reader digest would read waiting sorry got makes hesitant buy books listed humor buck buck \n",
            "Original Summary: would have been amusing in past \n",
            "Predicted Summary:  not bad\n",
            "BLEU score: 0.9671682101338347\n",
            "\n",
            "\n",
            "Original Text Review: start finish pulled sci fi world strange characters couple goes whole lot sexually charged situations cup tea made rush end \n",
            "Original Summary: stars \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.7400828044922853\n",
            "\n",
            "\n",
            "Original Text Review: thought beautiful love story bj perfect top cute little ending \n",
            "Original Summary: cute little love story \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.7952707287670506\n",
            "\n",
            "\n",
            "Original Text Review: well written thriller interesting premise convoluted times nice twists unusual thought provoking ending gritty thriller people ran harry \n",
            "Original Summary: murder mystery \n",
            "Predicted Summary:  great read\n",
            "BLEU score: 0.8210967436686386\n",
            "\n",
            "\n",
            "Original Text Review: book well written beginning actually understood back forth ending complete disappointment felt rushed incomplete \n",
            "Original Summary: good read \n",
            "Predicted Summary:  not bad\n",
            "BLEU score: 0.8408964152537145\n",
            "\n",
            "\n",
            "Original Text Review: wedding bring two people together great shifter story alpha female wolf cannot shift \n",
            "Original Summary: we are \n",
            "Predicted Summary:  great read\n",
            "BLEU score: 0.7765453555044466\n",
            "\n",
            "\n",
            "Original Text Review: fully original good read fast read would liked little epilogue update knew story turned little \n",
            "Original Summary: it is okay \n",
            "Predicted Summary:  good read\n",
            "BLEU score: 0.7400828044922853\n",
            "\n",
            "\n",
            "Original Text Review: final book good right ties three books together unit keep course trained risk life lives others needs \n",
            "Original Summary: great book \n",
            "Predicted Summary:  great read\n",
            "BLEU score: 0.8593887047640296\n",
            "\n",
            "\n",
            "Average of All BLEU Scores: 0.7711779604812647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTkaYNjHW4lC",
        "colab_type": "text"
      },
      "source": [
        "#Next Steps and Ways to Improve the Model’s Performance\n",
        "\n",
        "**Increase the training dataset** size and build the model. The generalization capability of a deep learning model enhances with an increase in the training dataset size.\n",
        "\n",
        "Try implementing **Bi-Directional LSTM** which is capable of capturing the context from both the directions and results in a better context vector.\n",
        "\n",
        "Use the **beam search strategy** for decoding the test sequence instead of using the greedy approach (argmax).\n",
        "\n",
        "Implement **pointer-generator networks** and **coverage mechanisms**."
      ]
    }
  ]
}